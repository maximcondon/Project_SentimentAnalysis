{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "- The process of analysing online pieces of writing to determine the emotional tone they carry.\n",
    "- Tools categorize pieces of writing as positive, neutral, or negative.\n",
    "- Can be found on discussion forums, review sites, Twitter, Instagram, Facebook and other publicly available online sources\n",
    "\n",
    "### Sentiment analysis challenges\n",
    "Due to language complexity, sentiment analysis has to face at least a couple of issues.\n",
    "\n",
    "- **Contrastive conjunction**\n",
    "    - One problem a sentiment analysis tool has to face is contrastive conjunctions — they happen when one piece of writing (a sentence) consists of two contradictory words (both positive and negative).\n",
    "\n",
    "Example sentence: “The weather was terrible, but the hike was amazing!”\n",
    "\n",
    "- **Named-entity recognition**\n",
    "    - Another big problem sentiment analysis algorithms face is named-entity recognition. Words in context have different meaning.\n",
    "\n",
    "Does “Everest” refer to the mountain or to the movie?\n",
    "\n",
    "- **Anaphora resolution**\n",
    "Also known as pronoun resolution, describes the problem of references within a sentence: what a pronoun, or a noun refers to.\n",
    "\n",
    "Example sentence: “We went to the theater and went for a dinner. It was awful.”\n",
    "\n",
    "- **Sarcasm**\n",
    "Is there any sentiment analysis tool detecting sarcasm? Please recommend one!\n",
    "\n",
    "Example sentence: “I’m so happy the plane is delayed.”\n",
    "\n",
    "- **The Internet**\n",
    "It just so happens that any language used online takes its own form. The economy of language and the Internet as a medium result in poor spelling, abbreviations, acronyms, lack of capitals and poor grammar. Analyzing such pieces of writing may cause problems for sentiment analysis algorithms.\n",
    "\n",
    "### Sentiment analysis can be done on:\n",
    "- Document level – modeling long-term relationships\n",
    "- Sentence level – is there a sentiment, and which?\n",
    "- Aspect extraction – “great phone but crappy display” (difficult)\n",
    "\n",
    "### Lexical methods\n",
    "Early sentiment analysis used manually curated lists of good/bad words. This approach is now widely inferior to machine learning.\n",
    "\n",
    "### Classical Supervised Learning\n",
    "Bag-of-words and Naive Bayes work to some extent for simpler sentiment analysis tasks. Support Vector Machines are used to model complex content, with and without word embeddings.\n",
    "\n",
    "### Embedding methods\n",
    "Most modern sentiment analysis models are based on word embeddings. Popular architectures include:\n",
    "\n",
    "- LSTMs:\n",
    "    - Embedding -> LSTM -> Output layer\n",
    "- LSTM with Pooling\n",
    "    - Embedding -> LSTM -> MeanPool -> LogReg\n",
    "- Convolution\n",
    "    - Claim to be trained faster:\n",
    "    - Embedding -> Conv1D -> Conv1D -> Dense -> Output\n",
    "\n",
    "### Character-level embeddings\n",
    "Both LSTMs and Convolutions work not only with word embeddings, but also character-level embeddings. In that case, the input would be an integer for each character, and the weights of the embedding layer would be trained, too, instead of using pre-trained weights.\n",
    "\n",
    "## Sentiment Analysis - Two Options: Build your own, or use a handy python package!\n",
    "\n",
    "### We will try both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Build your own Sentiment Analysis model using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First setup our imports\n",
    "* We'll use the imdb dataset from keras\n",
    "* The data has already been preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding is contained in the Embedding layer\n",
    "- **It takes 3 arguments - the size of the vocab (input_dims), the no. of dimensions of each word embedding (output_dim), and the length of each document (input_length)**\n",
    "- **It outputs a 2D matrix, with rows equal to each word in the document, and columns equal to the number of dimensions in the word embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['I really didnt like it', 'it was amazing',\n",
    "        'it was great','as great as talking to nedra',\n",
    "        'waste of time', 'well worth it', 'awesome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1,1,1,0,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encode the text\n",
    "* We have to transform the text we give to the sentiment analysis networkd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 1 : Create a vocab_to_keys and keys_to_vocab list for each unique word in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talking',\n",
       " 'like',\n",
       " 'it',\n",
       " 'i',\n",
       " 'great',\n",
       " 'to',\n",
       " 'well',\n",
       " 'really',\n",
       " 'awesome',\n",
       " 'waste',\n",
       " 'as',\n",
       " 'nedra',\n",
       " 'of',\n",
       " 'amazing',\n",
       " 'didnt',\n",
       " 'was',\n",
       " 'time',\n",
       " 'worth']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = []\n",
    "max_length = 0\n",
    "\n",
    "for review in reviews:\n",
    "    review = review.lower().split()\n",
    "    for word in review:\n",
    "        vocab.append(word)\n",
    "        if len(review) > max_length:\n",
    "            max_length = len(review)\n",
    "            \n",
    "vocab = list(set(vocab))\n",
    "vocab_size = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = vocab_size + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Integer encode the words in each document\n",
    "\n",
    "- **Turn our words from words into numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_keys = {} # Key : word, value : unique id\n",
    "key_to_vocab = {} # Key : unique id, value : word\n",
    "\n",
    "# embedded_reviews = []\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "        vocab_to_keys[vocab[i]] = i+1\n",
    "        key_to_vocab[i+1] = vocab[i]\n",
    "\n",
    "# for review in reviews:\n",
    "#     review = review.lower().split()\n",
    "#     for i in range(len(review)):\n",
    "#         vocab_to_keys[vocab[i]] = i\n",
    "#         key_to_vocab[i] = vocab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'talking': 1,\n",
       " 'like': 2,\n",
       " 'it': 3,\n",
       " 'i': 4,\n",
       " 'great': 5,\n",
       " 'to': 6,\n",
       " 'well': 7,\n",
       " 'really': 8,\n",
       " 'awesome': 9,\n",
       " 'waste': 10,\n",
       " 'as': 11,\n",
       " 'nedra': 12,\n",
       " 'of': 13,\n",
       " 'amazing': 14,\n",
       " 'didnt': 15,\n",
       " 'was': 16,\n",
       " 'time': 17,\n",
       " 'worth': 18}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'talking',\n",
       " 2: 'like',\n",
       " 3: 'it',\n",
       " 4: 'i',\n",
       " 5: 'great',\n",
       " 6: 'to',\n",
       " 7: 'well',\n",
       " 8: 'really',\n",
       " 9: 'awesome',\n",
       " 10: 'waste',\n",
       " 11: 'as',\n",
       " 12: 'nedra',\n",
       " 13: 'of',\n",
       " 14: 'amazing',\n",
       " 15: 'didnt',\n",
       " 16: 'was',\n",
       " 17: 'time',\n",
       " 18: 'worth'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I really didnt like it',\n",
       " 'it was amazing',\n",
       " 'it was great',\n",
       " 'as great as talking to nedra',\n",
       " 'waste of time',\n",
       " 'well worth it',\n",
       " 'awesome']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 8, 15, 2, 3],\n",
       " [3, 16, 14],\n",
       " [3, 16, 5],\n",
       " [11, 5, 11, 1, 6, 12],\n",
       " [10, 13, 17],\n",
       " [7, 18, 3],\n",
       " [9]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs = [[vocab_to_keys[x] for x in review.lower().split()] for review in reviews]\n",
    "embedded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We've created two dictionaries with reference numbers!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncate and pad the review sequences\n",
    "* Every input has to have the same shape, in our case the first 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  8, 15,  2,  3,  0],\n",
       "       [ 3, 16, 14,  0,  0,  0],\n",
       "       [ 3, 16,  5,  0,  0,  0],\n",
       "       [11,  5, 11,  1,  6, 12],\n",
       "       [10, 13, 17,  0,  0,  0],\n",
       "       [ 7, 18,  3,  0,  0,  0],\n",
       "       [ 9,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs = sequence.pad_sequences(embedded_docs, maxlen=max_length, padding='post')\n",
    "padded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model on the training data\n",
    "* Fit the word embeddings from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 16, input_length=max_length)) # Embedding Layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.6942 - acc: 0.5714\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 466us/step - loss: 0.6899 - acc: 0.5714\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 787us/step - loss: 0.6856 - acc: 0.7143\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 487us/step - loss: 0.6816 - acc: 0.8571\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 436us/step - loss: 0.6773 - acc: 0.8571\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 837us/step - loss: 0.6730 - acc: 0.8571\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 498us/step - loss: 0.6688 - acc: 0.8571\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 525us/step - loss: 0.6645 - acc: 0.8571\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 899us/step - loss: 0.6602 - acc: 0.8571\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 818us/step - loss: 0.6560 - acc: 0.8571\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6517 - acc: 0.8571\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.6475 - acc: 0.8571\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6432 - acc: 0.8571\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6390 - acc: 0.8571\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6347 - acc: 0.8571\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 536us/step - loss: 0.6305 - acc: 0.8571\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 630us/step - loss: 0.6262 - acc: 0.8571\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 389us/step - loss: 0.6220 - acc: 0.8571\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6177 - acc: 0.8571\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.6134 - acc: 0.8571\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 603us/step - loss: 0.6091 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 441us/step - loss: 0.6048 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 919us/step - loss: 0.6005 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 449us/step - loss: 0.5962 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 406us/step - loss: 0.5918 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 654us/step - loss: 0.5875 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 596us/step - loss: 0.5831 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 446us/step - loss: 0.5787 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 470us/step - loss: 0.5743 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 653us/step - loss: 0.5699 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 441us/step - loss: 0.5654 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 470us/step - loss: 0.5610 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 543us/step - loss: 0.5565 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 568us/step - loss: 0.5519 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 380us/step - loss: 0.5474 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 613us/step - loss: 0.5428 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 681us/step - loss: 0.5381 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 661us/step - loss: 0.5335 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 433us/step - loss: 0.5289 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 549us/step - loss: 0.5242 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 509us/step - loss: 0.5195 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 524us/step - loss: 0.5148 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 950us/step - loss: 0.5101 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 579us/step - loss: 0.5053 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 456us/step - loss: 0.5006 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 471us/step - loss: 0.4958 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 576us/step - loss: 0.4910 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 456us/step - loss: 0.4862 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 722us/step - loss: 0.4814 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 464us/step - loss: 0.4766 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1253b7898>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = padded_docs\n",
    "y = labels\n",
    "\n",
    "model.fit(X, y, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6, 16)             304       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 401\n",
      "Trainable params: 401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "7/7 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4718186557292938, 1.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X,y,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try and predict the sentiment of new text you feed in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the text in the same way\n",
    "#### And predict the sentiment of the sentence against the model's prediction of the padded_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 6\n",
    "new_reviews = ['it was really amazing nedra', 'it was great like', 'amazing waste of time']\n",
    "embedded_docs = [[vocab_to_keys[x] for x in review.lower().split()] for review in new_reviews]\n",
    "padded_docs = sequence.pad_sequences(embedded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 16,  8, 14, 12,  0],\n",
       "       [ 3, 16,  5,  2,  0,  0],\n",
       "       [14, 10, 13, 17,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_docs\n",
    "test_labels = [1,1,0]\n",
    "ypred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5803242],\n",
       "       [0.6323064],\n",
       "       [0.5480741]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is our model good? \n",
    "* Discuss why\n",
    "* Discuss bias\n",
    "* Discuss how to circumvent training - 2 solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the model is confused by the final review, 'amazing' makes it think it's a positive but then it is juxtaposed with 'waste of time'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER\n",
    "* We will be using **Vader**, a sentiment analysis package trained on social media data\n",
    "* It is a good out of the box tool & v easy to use\n",
    "* Good at handling sublte sentiment signs, e.g:\n",
    "    * good!!! > good!\n",
    "    * omg so good > good\n",
    "    * GOOD > good\n",
    "    * good :) > good\n",
    "    \n",
    "*Pip install vaderSentiment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.545, 'neu': 0.455, 'pos': 0.0, 'compound': -0.5574}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores('all humans are shit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'compound': 0.6249}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'hi this string is awesome'\n",
    "analyzer.polarity_scores(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really didnt like it {'neg': 0.443, 'neu': 0.557, 'pos': 0.0, 'compound': -0.3374}\n",
      "\n",
      "it was amazing {'neg': 0.0, 'neu': 0.345, 'pos': 0.655, 'compound': 0.5859}\n",
      "\n",
      "it was great {'neg': 0.0, 'neu': 0.328, 'pos': 0.672, 'compound': 0.6249}\n",
      "\n",
      "as great as talking to nedra {'neg': 0.0, 'neu': 0.549, 'pos': 0.451, 'compound': 0.6249}\n",
      "\n",
      "waste of time {'neg': 0.583, 'neu': 0.417, 'pos': 0.0, 'compound': -0.4215}\n",
      "\n",
      "well worth it {'neg': 0.0, 'neu': 0.2, 'pos': 0.8, 'compound': 0.4588}\n",
      "\n",
      "awesome {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6249}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for review in reviews:\n",
    "    print(review, analyzer.polarity_scores(review))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores('lecture over')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
